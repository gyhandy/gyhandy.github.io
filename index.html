<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
    <head>
    <meta name="google-site-verification" content="eoPCGBBxDIK0Ff9Dk_dXsuHMTNzzSEZMbsfO4zriBK8" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="keywords" content="Yunhao Ge, NVIDIA, USC ">
    <meta name="description" content="Yunhao Ge's home page">
<!--    <link href="main.css" media="all" rel="stylesheet">-->
    <link rel="stylesheet" href="jemdoc.css" type="text/css">
    <title>Yunhao (Andy) Ge</title>
    </head>

<body>


<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">
					<h1>Yunhao (Andy) Ge  &nbsp &nbsp  葛云皓</h1>
<!--					<h1>葛云皓</h1><h1>-->
				</div>

				<h3>Research Scientist @ NVIDIA <br><br>
<!--					Visiting Ph.D. Student @ Stanford University <br><br>-->
<!--				Student Researcher @ Google Research <br><br>-->
<!--				Amazon Fellow with Amazon ML Fellowship (2022-2023)-->
				</h3>
<!--				<h3>Student Researcher @ Google Research</h3>-->
<!--				<h3>Amazon Fellow with Amazon ML Fellowship (2022-2023)</h3>-->
<!--                <h3>University of Southern California</h3>-->
				<p>
<!--					Department of Computer Science @-->
<!--					University of Southern California <br>-->
<!--					Rm B06, Hedco Neurosciences Building, 3641 Watt Way, Los Angeles, CA 90089-2520, USA <br>-->

					Email: yunhaog at nvidia dot com

				</p>
				<p> <a href="https://scholar.google.ca/citations?user=QhjGr4oAAAAJ&hl=en"><img src="./pics/google_scholar3.png" height="40px" style="margin-bottom:-3px"></a>
					<a href="https://github.com/gyhandy"><img src="./pics/github_s.jpg" height="40px" style="margin-bottom:-3px"></a>
                    <a href="https://www.linkedin.com/in/yunhao-ge-720727135"><img src="./pics/LinkedIn2.png" height="40px" style="margin-bottom:-3px"></a>
                    <a href="files/CV_YunhaoGe.pdf"><img src="./pics/cv2.png" height="40px" style="margin-bottom:-3px"></a>
					&nbsp &nbsp
					<a href="#C1">[<font size="3" color="#CB4335"><b>About Me</b></font>] </a>
					<a href="#C2">[<font size="3" color="#CB4335"><b>News</b></font>]</a>
					<a href="#C3">[<font size="3" color="#CB4335"><b>Publications</b></font>]</a>
					<a href="#C4">[<font size="3" color="#CB4335"><b>Experience</b></font>]</a>
<!--					<a href="#C5">[<font size="3" color="#CB4335"><b>Mentorship</b></font>]</a>-->
					</li>
				</p>
			</td>
			<td>
				<img src="pics/Andy3.png" border="0" width="200"><br>
<!--				<img src="pics/Yunhao Ge3.jpg" border="0" width="230"><br>-->
<!--				<img src="pics/cover.png" border="0" width="540"><br>-->
			</td>
		</tr><tr>
	</tr></tbody>
</table>

<!--<style>-->
<!--ul-->
<!--{-->
<!--	list-style-type:none;-->
<!--	margin:0;-->
<!--	padding:0;-->
<!--}-->
<!--li a:hover {-->
<!--    background-color: #555;-->
<!--    color: white;-->
<!--}-->

<!--</style>-->


<!--   #0F73B6-->

<h2><a id="C1" ><font color="#CB4335">About Me</font></a></h2>
<p>
		I am a Research Scientist at <a  href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a>, working on <a  href="https://research.nvidia.com/labs/dir/">Generative AI</a>. I received my Ph.D. in
	 Computer Science from <a  href="https://www.cs.usc.edu/">University of Southern California</a> advised by Prof. <a  target="_blank" href=https://scholar.google.com/citations?user=xhUvqK8AAAAJ&hl=en target="_blank" rel="external">Laurent Itti</a>,
	and was honored with the <a  target="_blank" href=https://www.amazon.science/latest-news/amazon-and-usc-name-three-new-ml-fellows target="_blank" rel="external"> Amazon ML Fellowship</a>.
	I was a Visiting Ph.D. Student at <a  href="https://svl.stanford.edu/">Stanford Vision and Learning Lab (SVL)</a> advised by Prof. <a  target="_blank" href=https://jiajunwu.com/ target="_blank" rel="external">Jiajun Wu</a>.
<!--	I am a Ph.D. Candidate in the CS Department at <a  href="https://www.cs.usc.edu/">University of Southern California</a>,-->
<!--	 advised by Prof. <a  target="_blank" href=https://scholar.google.com/citations?user=xhUvqK8AAAAJ&hl=en target="_blank" rel="external">Laurent Itti</a>.-->
<!--	I am also a Visiting PhD Student at <a  href="https://svl.stanford.edu/">Stanford Vision and Learning Lab (SVL)</a> advised by Prof. <a  target="_blank" href=https://jiajunwu.com/ target="_blank" rel="external">Jiajun Wu</a>.-->

	I have broad research interests in Computer Vision and Natural Language Processing, with a recent focus on building foundation models for Multimodal-Large Language Models and Text-guided 2D/3D Generation.
<!--	My primary research interest lies in controllable data generation, intending to use generated dataset train AI models that can effectively perceive, understand, interact with, and reason about the physical world.-->
<!--	I'm interested in learning controllable data generation to train AI models to perceive, understand, interact, and reason the physical world. My current research focuses include:-->
<!--		I'm interested in how could human efficiently teach AI to learn the human ability to perceive, understand, interact, and reason the physical world. My current research focuses include:-->
<!--	I'm interested in Machine Learning, Computer vision, and their applications towards Trustworthy, Human-like AI and Data-centric AI. My current research focuses include:-->
<ul>
<!--		<li>Controllable Data Generation: <i>[Learning to generate]</i> Use generative models and neural renderers to synthesize realistic and physically plausible data automatically. <i>[Generating to learn]</i> AI models trained with synthetic data can solve real-world Vision and Robotics tasks.</li>-->
<!--       <li>Vision Language Models, Lifelong Learning, and Reliable Deep Learning </li>-->
	<!--	<li>Large Multimodal Models (LMM), Large Language Models(LLM), Lifelong Learning, and Reliable Deep Learning </li>-->
	<!--	<li>Vision‑Language Models, Lifelong Learning, Visual Reasoning and Reliable Deep Learning </li>-->
<!--	<li>Controllable Data Generation: using generative models and neural renderer to automatically synthesize realistic and physical plausible data to train AI models to solve real-world Computer Vision and Robotics problems</li>-->
<!--	<li>Vision‑Language Models, Lifelong Learning, Visual Reasoning and Reliable Deep Learning </li>-->
<!--	<li>Human-inspired Learning Algorithm (Lifelong Learning,  Multi-modal Models, Visual Reasoning) </li>-->

<!--	<li>Learning from Synthetic Data (Sim2Real): using neural renderer (NeRF, Stable Diffusion) and simulation to synthesize realistic and physical plausible data to solve real-world Computer Vision and Robotics problems-->
<!--		   with minimal human supervision </li>-->
<!--	<li>Reliable Deep Learning (Robustness, Out-of-distribution (OOD) Detection, Interpretability)</li>-->
<!--		<li>Reliable Deep Learning (Robustness, Out-of-distribution (OOD) Detection, Interpretability)</li>-->
<!--	  <li>Synthetic data for Vision/Robotics (Sim2Real): using neural renderer (NeRF, DALL-E / Stable Diffusion, GAN) to synthesize realistic and physical plausible data to solve real-world Computer Vision and Robotics problems-->
<!--		   with minimal supervision </li>-->
<!--	  <li>Human-inspired Learning Algorithm (Continual Learning,  Multi-modal Models, Visual Reasoning) </li>-->
<!--	  <li>Trustworthy AI (Interpretability, Robustness, Out-of-distribution (OOD) Detection, Human-to-AI Knowledge Exchange, Causality)</li>-->
<!--	  <li>Data-centric AI (Sim2Real): using neural renderer (NeRF, DALL-E / Stable Diffusion, GAN) to synthesize realistic and physical plausible data to solve real-world CV problems-->
<!--		  (classification, detection, segmentation) with minimal supervision </li>-->
<!--	  <li>Human-like AI to simulate human cognitive learning ability (Continual Learning,  Multi-modal Models (CLIP), Visual Reasoning) </li>-->

<!--		I'm interested in Machine Learning, Computer vision, and their applications towards Human-centric / Humanoid AI and Data-centric AI. My current research focuses include:-->
<!--<ul>-->
<!--	  <li>Human-centric properties of AI models (Causal Explainability, Robustness, Domain Adaptation, Out-of-distribution Detection (OOD), Human-to-AI Knowledge Exchange)</li>-->
<!--	  <li>Simulate human cognitive learning ability (Continual Learning,  Multi-modal (CLIP), Imagination, Reasoning, Visual Recognition) </li>-->
<!--	  <li>Data-centric AI: using synthetic data and neural renderer (NeRF, DALL-E, GAN, VAE) to solve real-world computer vision problems-->
<!--		  (classification, detection, segmentation) with minimal supervision </li>-->
<!--<p>I am now a Ph.D. Candidate in the Department of CS at <a  href="https://www.cs.usc.edu/">University of Southern California</a> (USC) and a member of <a  href="http://ilab.usc.edu/">iLab</a>,-->
<!--   working with Prof. <a  target="_blank" href=https://scholar.google.com/citations?user=xhUvqK8AAAAJ&hl=en target="_blank" rel="external">Laurent Itti</a>.-->
<!--		I'm interested in Machine Learning, Computer vision, and their applications towards Human-centric / Humanoid AI and Data-centric AI. My current research focuses include:-->
<!--<ul>-->
<!--	  <li>Human-centric properties of AI models (Causal Explainable AI, Human-to-AI / AI-to-AI Knowledge Exchange, Domain Adaptation, Out-of-distribution Detection (OOD))</li>-->
<!--	  <li>Simulate human cognitive learning ability (Continual Learning, Imagination, Reasoning, Visual Recognition) </li>-->
<!--	  <li>How generative models (NeRF, GAN, VAE) and multi-modal models (DALL-E, CLIP) help downstream discriminative models (detection, segmentation) </li>-->
<!--	  <li>Causal Explainable AI ((1) Reveal reasoning logic and causality of Neural Networks (NN) (2) Use explanation / causality as feedback to help improve the performance of the original NN.) </li>-->
	<!--	  <li>Understanding AI models beyond accuracy (disentangled representation learning, human-NN knowledge exchange, steerability, generalization, domain adaptation, and bias)</li>-->
	<!--	  <li>Humanoid Neural Network (simulating human cognitive learning ability (Imagination, Reasoning, Visual Recognition, Continual Learning) by using various learning algorithms (Generative models, Representation Learning, Graph Neural Network, etc.)</li>-->
<!--	  <li>Effortless AI (how generative models (NeRF, DALL-E, GAN, VAE) reduce human effort and boost discriminative models)</li>-->
	</ul>

<!--	I also work closely with-->
<!--	Dr. <a  target="_blank" href=http://vibhavvineet.info/ target="_blank" rel="external"> Vibhav Vineet </a> (<a  target="_blank" href=https://www.microsoft.com/en-us/research/ target="_blank" rel="external">Microsoft Research</a>),-->
<!--	Dr. <a  target="_blank" href=https://jessieren.github.io/ target="_blank" rel="external"> Jie Ren </a> (<a  target="_blank" href=https://research.google/teams/brain/ target="_blank" rel="external">Google Brain</a>),-->
<!--	Dr. <a  target="_blank" href=https://scholar.google.com/citations?user=j64_S3EAAAAJ&hl=en rel="external"> Jiaping Zhao </a> (<a  target="_blank" href=https://research.google/ target="_blank" rel="external">Google Research</a>),-->
<!--	and Dr. <a  target="_blank" href=http://wuziyan.com/ target="_blank" rel="external">Ziyan Wu</a> (<a  target="_blank" href="https://www.uii-ai.com/en/" target="_blank" rel="external">UII America</a>).-->


	Previously, I was fortunate to intern/work at <a  target="_blank" href=https://research.google/ target="_blank" rel="external"> Google Research</a>, <a  target="_blank" href=https://cloud.google.com/ target="_blank" rel="external"> Google Cloud AI</a>,
<a  target="_blank" href="https://www.microsoft.com/en-us/research/research-area/computer-vision/?facet%5Btax%5D%5Bmsr-research-area%5D%5B0%5D=13562&sort_by=most-recent" target="_blank" rel="external">Microsoft Research</a>,
<a  target="_blank" href="https://www.uii-ai.com/en/" target="_blank" rel="external">United Imaging Intelligence</a>,
	and <a  target="_blank" href="http://flexiv.com/" target="_blank" rel="external">Flexiv Robotics</a>. Before that, I got my M.Sc. degree at Robotics Institute at <a  target="_blank" href="http://en.sjtu.edu.cn/">Shanghai Jiao Tong University</a>.


</p>

<p> <a href="https://www.nvidia.com/en-us/research/"><img src="./pics/nvidia.png" height="70px" style="margin-bottom:-6px"></a>
	<a href="https://research.google/"><img src="./pics/google_logo.png" height="50px" style="margin-bottom:2px"></a>&nbsp
	<a href="https://trustedai.usc.edu/"><img src="./pics/Amazon-logo.png" height="40px" style="margin-bottom:3px"></a>&nbsp
	<a href="https://www.microsoft.com/en-us/research/"><img src="./pics/microsoft_logo.jfif" height="55px" style="margin-bottom:-1px"></a>&nbsp &nbsp &nbsp
	<a href="https://www.uii-ai.com/en/"><img src="./pics/UII_logo33.png" height="55px" style="margin-bottom:-1px"></a>&nbsp &nbsp &nbsp
	<a href="http://flexiv.com/"><img src="./pics/flexiv_logo.jfif" height="60px" style="margin-bottom:-3px"></a>
	&nbsp &nbsp &nbsp &nbsp &nbsp &nbsp
	<a href="https://svl.stanford.edu/"><img src="./pics/stanford.png" height="76px" style="margin-bottom:-9px"></a>&nbsp &nbsp &nbsp
	<a href="https://www.cs.usc.edu/"><img src="./pics/USC_logo.png" height="60px" style="margin-bottom:-3px"></a>&nbsp &nbsp &nbsp
	<a href="http://en.sjtu.edu.cn/"><img src="./pics/sjtu_logo.png" height="60px" style="margin-bottom:-3px"></a>&nbsp &nbsp &nbsp
	<a href="http://www.en.sdu.edu.cn/"><img src="./pics/sdu_logo.jpg" height="62px" style="margin-bottom:-3px"></a>



</p>


<!--	Previously, I was a student researcher at <a  target="_blank" href=https://research.google/ target="_blank" rel="external"> Google Research </a> working with-->
<!--	Dr. <a  target="_blank" href=https://scholar.google.com/citations?user=j64_S3EAAAAJ&hl=en rel="external"> Jiaping Zhao </a>,-->
<!--	Dr. <a  target="_blank" href=https://jessieren.github.io/ target="_blank" rel="external"> Jie Ren </a>,-->
<!--	Dr. <a  target="_blank" href=http://www.gatsby.ucl.ac.uk/~balaji/ target="_blank" rel="external"> Balaji Lakshminarayanan </a>-->
<!--    and   Prof. <a  target="_blank" href=http://faculty.ucmerced.edu/mhyang/ target="_blank" rel="external"> Ming-Hsuan Yang</a>; a student researcher at <a  target="_blank" href=https://cloud.google.com/ target="_blank" rel="external"> Google Cloud AI </a> working with Dr. <a  target="_blank" href=https://sercanarik.com/ rel="external"> Sercan Arik </a>-->
<!--        and Dr. <a  target="_blank" href=https://sites.google.com/view/jinsungyoon target="_blank" rel="external"> Jinsung Yoon </a>; a research intern at-->
<!--<a  target="_blank" href="https://www.microsoft.com/en-us/research/research-area/computer-vision/?facet%5Btax%5D%5Bmsr-research-area%5D%5B0%5D=13562&sort_by=most-recent" target="_blank" rel="external">Microsoft Research</a></li>-->
<!--		  working with Dr. <a  target="_blank" href=http://vibhavvineet.info/ target="_blank" rel="external"> Vibhav Vineet </a> and-->
<!--and Dr. <a  target="_blank" href=https://neelj.com/ target="_blank" rel="external"> Neel Joshi </a>; and a research intern at-->
<!-- <a  target="_blank" href="https://www.uii-ai.com/en/" target="_blank" rel="external">UII America</a></li> working with-->
<!--        Dr. <a  target="_blank" href=http://wuziyan.com/ target="_blank" rel="external"> Ziyan Wu </a>-->
<!--            and Dr. <a  target="_blank" href=https://karanams.github.io/ target="_blank" rel="external"> Srikrishna Karanam </a>. <br>-->
<!--</p>-->
<!--	Before that, I got my M.Sc. degree at Robotics Institute at <a  target="_blank" href="http://en.sjtu.edu.cn/">Shanghai Jiao Tong University</a>.-->
<!--	and B.Eng. degree of Mechatronics at <a  target="_blank" href="http://www.en.sdu.edu.cn/"> Shandong University</a>. </p>-->
<!-- I have been lucky to work with Prof. <a  target="_blank" href=https://scholar.google.com/citations?user=v6VYQC8AAAAJ&hl=en target="_blank" rel="external"> Dinggang Shen </a> and-->
<!--	Dr. <a  target="_blank" href=https://scholar.google.com/citations?user=INL-unYAAAAJ&hl=en target="_blank" rel="external"> Shu Liao </a> during my internship at <a  target="_blank" href="https://www.uii-ai.com/en/" target="_blank" rel="external">United Imaging Intelligence</a>,-->
<!--	as well as Prof. <a  target="_blank" href=https://www.mvig.org/ target="_blank" rel="external"> Cewu Lu </a> and-->
<!--	Dr. <a  target="_blank" href=https://www.linkedin.com/in/shuyun-chung-67b5337a/ target="_blank" rel="external"> Shuyun Chong</a> when I was a Computer Vision Engineer at <a  target="_blank" href="http://flexiv.com/" target="_blank" rel="external">Flexiv Robotics</a>.-->
<!--</p>-->

<!--<ul>-->
<!--	  <li>Causal Explainable AI ((1) Understanding reasoning logic and causality of Neural Networks (NN) (2) Use explanation as feedback to help improve the performance of the original NN.) </li>-->
<!--	  <li>Interpretable human-AI interaction (understanding AI models beyond accuracy, such as disentangled representation learning, human-NN knowledge exchange, steerability, generalization, fairness and bias)</li>-->
<!--	  <li>Humanoid Neural Network (simulating human cognitive learning ability (Imagination, Reasoning, Visual Recognition) by using various learning algorithms (Generative models, Representation Learning, Graph Neural Network, Contrastive Learning, etc.)</li>-->
<!--	  <li>Effortless AI (how generative models reduce human effort and boost discriminative models)</li>-->
<!--	</ul>-->

<!--	I'm interested in Machine Learning, Computer vision, and their applications towards Artificial General Intelligence (AGI). My current research focuses include:-->
<!--<ul>-->
<!--	  <li>interpretable human-AI interaction (interpretability, steerability, disentangled representation learning)</li>-->
<!--	  <li>generative models (data augmentation, how generative models boost discriminative models)</li>-->
<!--	  <li>graph neural networks (structure and relationship learning)</li>-->
<!--	  <li>visual reasoning, attention and saliency (cognitive learning, eye tracking)</li>-->
<!--	</ul>-->
<!--<p>My research interests lie in Machine Learning, Computer vision, and AGI. Currently, I am focusing on simulating Cognitive Baby Learning (Imagination, Reasoning, Attention)-->
<!--by using various learning algorithms (Representation Learning, Generative models, GNN, Reinforcement Learning,  Meta-Learning, etc.).</p>-->
<!--<p> <b>Research opportunities</b>: I am happy to collaboration. If you are interested, please send me an email.-->
<!--	I especially encourage USC master/undergraduate students who want to involve exciting projects targeting top-tier conferences/journals to reach out.-->
<!--</p>-->
<!--I am happy to collaborate and/or answer questions about my research.-->
<!--<p>My research interests lie in Computer vision, Robotics and General AI. Currently, I am-->
<!--focusing on simulating baby learning (Reasoning, Attention, Imagination) by using various learning algorithms (Representation Learning, Adversarial Learning,-->
<!--Meta Learning, GNN, Reinforcement Learning, etc.).</p>-->
<br>




<h2><a id="C2" ><font color="#CB4335">News & Updates</font></a></h2>
<ul>

<div style="height:400px;width:fit-content;overflow:auto;background:#FFFFFF;">

	<li>
		<p>[2024/12/21] One paper on <b>LLM Agent Tool Use</b> for 2D/3D captioning is accepted by <b>CVPR 2024</b>.
			</p>
	<li>
		<p>[2023/12/21] We release the paper and code of <a  href="https://briannlongzhao.github.io/DreamDistribution/">DreamDistribution </a> for personalized 2D/3D generation.
			</p>
	<li>
		<p>[2023/12/18] Starting a new journey at <a  href="https://research.nvidia.com/labs/dir/">NVIDIA Research </a> as a Research Scientist.
			</p>
	<li>
		<p>[2023/09/23] One paper on <b>3D Copy-Paste</b> is accepted by <b>NeurIPS 2023</b>.
			</p>
	<li>
		<p>[2023/07/26] One paper on <b>Lifelong (Continual) Learning</b> is accepted by <b>ICCV 2023</b>.
			</p>
	<li>
		<p>[2023/05/09] One paper on <b>Shared Knowledge Lifelong Learning</b>, a new Lifelong Learning paradigm, is accepted by <b>TMLR</b>.
			</p>
	<li>
		<p>[2023/02/27] One paper on <b>Multi-modal models' Robustness and Generalization</b> is accepted by <b>CVPR</b> 2023.
			</p>
	</li>

	<li>
		<p>[2022/12/01] Starting a new journey at <a  href="https://svl.stanford.edu/">Stanford Vision and Learning (SVL) Lab </a> as a Visiting Student Researcher, advised by Prof. <a  target="_blank" href=https://jiajunwu.com/ target="_blank" rel="external">Jiajun Wu</a>.
			</p>
	</li>
	<li>
		<p>[2022/08/16] I was awarded the Amazon ML Fellowship (2022-2023), and will be an Amazon Fellow at
			<a  target="_blank" href=https://trustedai.usc.edu/ target="_blank" rel="external"> USC + Amazon Center on Secure & Trusted Machine Learning</a>. Thank you Amazon! </p>
	</li>
	<li>
		<p>[2022/08/16] One paper on <b>Disentangled and Convex Representation learning</b> is accepted by <b>WACV</b> 2023, code is coming soon.</p>
	</li>
	<li>
		<p>[2022/07/03] Two papers on <b>NeRF</b> and <b>Humanoid Neural Network</b> are accepted by <b>ECCV</b> 2022, code are released.</p>
	</li>
	<li>
		<p>[2022/05/31] I will be joining  <a  target="_blank" href=https://research.google/ target="_blank" rel="external"> Google Research </a>  as a student researcher, advised by
			Dr. <a  target="_blank" href=https://scholar.google.com/citations?user=j64_S3EAAAAJ&hl=en rel="external"> Jiaping Zhao </a>,
	Dr. <a  target="_blank" href=https://jessieren.github.io/ target="_blank" rel="external"> Jie Ren </a>,
	Dr. <a  target="_blank" href=http://www.gatsby.ucl.ac.uk/~balaji/ target="_blank" rel="external"> Balaji Lakshminarayanan </a>
    and   Prof. <a  target="_blank" href=http://faculty.ucmerced.edu/mhyang/ target="_blank" rel="external"> Ming-Hsuan Yang. </a></li></p>
	</li>
	<li>
		<p>[2022/01/20] Finally passed my qual exam and officially became a PhD <b>Candidate</b> now.</p>
	</li>
	<li>
		<p>[2021/08/23] I will be joining  <a  target="_blank" href=https://cloud.google.com/ target="_blank" rel="external"> Google Cloud AI </a>  as a student researcher, advised by
        Dr. <a  target="_blank" href=https://sercanarik.com/ rel="external"> Sercan Arik </a>
        and Dr. <a  target="_blank" href=https://sites.google.com/view/jinsungyoon target="_blank" rel="external"> Jinsung Yoon </a></li></p>
	</li>
	<li>
		<p>[2021/07/15] <a  target="_blank" href=https://viterbischool.usc.edu/news/2021/07/usc-researchers-enable-ai-to-use-its-imagination/ target="_blank" rel="external"> USC News </a>,
			<a  target="_blank" href=https://techxplore.com/news/2021-07-enabling-artificial-intelligence.html target="_blank" rel="external"> Tech Xplore </a>,
			<a  target="_blank" href=https://www.technologynetworks.com/informatics/news/enabling-ai-to-use-its-imagination-350886 target="_blank" rel="external"> Technology Networks </a>
			and other media pressed our <b>ICLR</b> 2021 paper: <a href="https://arxiv.org/pdf/2009.06586.pdf" target="_blank">Group-Supervised Learning</a> (Enabling the 'imagination' of artificial intelligence)</p>
	</li>
	<li>
		<p>[2021/05/17] I will be joining Computer Vision Group at <a  target="_blank" href=https://www.microsoft.com/en-us/research/ target="_blank" rel="external"> Microsoft Research </a>  Redmond as a research intern in summer 2021, advised by
        Dr. <a  target="_blank" href=http://vibhavvineet.info/ target="_blank" rel="external"> Vibhav Vineet </a>
        and Dr. <a  target="_blank" href=https://neelj.com/ target="_blank" rel="external"> Neel Joshi </a></li></p>
	</li>
<!--	<li>-->
<!--		<p>[2021/05/08] Serving as a reviewer for NeurIPS 2021 and ICLR 2022!</p>-->
<!--	</li>-->
	<li>
		<p>[2021/04/07] Releasing <a  target="_blank" href=http://ilab.usc.edu/datasets/i2sg target="_blank" rel="external">Img2SceneGraph</a>,
		a pipeline that transfers images to scene graphs with node attributes!
		Welcome to <a  target="_blank" href=http://ilab.usc.edu/datasets/i2sg target="_blank" rel="external"> Download </a> and try!</p>
	</li>
	<li>
		<p>[2021/04/02] One paper (Graph Autoencoder for Graph Compression and Representation Learning)
		was accepted by Neural Compression Workshop @<b>ICLR</b> 2021 as <strong style="color:blue">Spotlight</strong>!</p>
	</li>
	<li>
		<p>[2021/02/28] One paper (A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts)
		was accepted by <b>CVPR</b> 2021!</p>
	</li>
	<li>
		<p>[2021/01/16] One paper (Beneficial Perturbation Network for designing general adaptive artificial intelligence systems)
		was accepted by <b>TNNLS</b>!</p>
	</li>
	<li>
		<p>[2021/01/12] One paper (Zero-shot Synthesis with Group-Supervised Learning) was accepted by <b>ICLR</b> 2021!</p>
	</li>
	<li>
		<p>[2020/09/14] <a  target="_blank" href=http://ilab.usc.edu/datasets/fonts target="_blank" rel="external"> Fonts dataset </a> was proposed for fast testing and idea iteration on disentangled representation learning and zero-shot synthesis.
			Welcome to <a  target="_blank" href=http://ilab.usc.edu/datasets/fonts target="_blank" rel="external"> Download </a> and try!</p>
	</li>
	<li>
		<p>[2020/07/02] One paper (Pose Augmentation: Class-agnostic Object Pose Transformation) was accepted by <b>ECCV</b> 2020!</p>
	</li>
	<li>
		<p>[2020/05/12] I will be joining UII America as a research intern in summer 2020, advised by
        Dr. <a  target="_blank" href=http://wuziyan.com/ target="_blank" rel="external"> Ziyan Wu </a>
        and Dr. <a  target="_blank" href=https://karanams.github.io/ target="_blank" rel="external"> Srikrishna Karanam </a></li></p>
	</li>
	<li>
		<p>[2019/08/12] I will be joining USC CS Ph.D. Program in fall 2019, advised by
        Prof. <a  target="_blank" href=http://ilab.usc.edu/itti/ target="_blank" rel="external">Laurent Itti</a>.</p>
	</li>
	<li>
		<p>[2019/07/01] One paper (Synthesis and inpainting-based MR-CT registration) was accepted by <b>MICCAI</b> 2019.</p>
	</li>
	<li>
		<p>[2019/03/01] One paper (Unpaired Whole-Body Mr to CT Synthesis) was accepted by <b>ISBI</b> 2019.</p>
	</li>

</div>
</ul>
<br>
<!--<h2>Education</h2>-->
<!--<hr>-->
<!--  <img id="school_logo" src="./pics/USC_logo.png">-->
<!--  <h4> University of Southern University, Los Angeles, USA (Aug. 2019 - present)</h4>-->
<!--  <ul>-->
<!--	<li>-->
<!--	  <b>PhD of Computer Science</b>, ilab @ Computer Science Department</li>-->
<!--	<li>Major Orientation: Deep Learning and Reinforcement learning for object recognition <br />Baby learning inspired causal inference </li>-->
<!--	<li>Annenberg Graduate Fellowship at University of Southern California </li>-->
<!--  </ul>-->
<!--  -->
<!--  <img id="school_logo" src="./pics/sjtu_logo.png">-->
<!--  <h4> Shanghai Jiao Tong University, Shanghai, China (Sep. 2016 - Jun. 2019)</h4>-->
<!--  <ul>-->
<!--	<li>-->
<!--	  <b>Master of Science(MSc)</b>, ROBOTICS AND INTELLIGENCE GROUP</li>-->
<!--	<li>Major Orientation: Deep Learning for Medical Image Computing(Computer Vison)<br />deep learning for Robotics and Machine Vision </li>-->
<!--	<li>Overall performance ranked: 6/210 </li>-->
<!--	<li>Excellent master's thesis "Automatic focusing and global precise imaging of <br>pathological microscope based on convolutional neural network "</li>-->
<!--  </ul>-->
<!--  -->
<!--  <img id="school_logo" src="./pics/sdu_logo.jpg">-->
<!--  <h4> Shandong University, Jinan, Shandong, China (Sep. 2012 - Jun. 2016)</h4>-->
<!--  <ul>-->
<!--	<li>-->
<!--	  <b>Bachelor of Engineering(B.Eng)</b>, MECHATRONICS </li>-->
<!--	<li>Major Orientation: Robotics Intelligent Control and Bionic Robotics</li>-->
<!--	<li>Overall performance ranked:-->
<!--	  <strong style="color:black">1/66</strong>   GPA: Overall: 86.08/100 | Major: 93.38/100 </li>-->
<!--  </ul>-->

<!--<h2><a id="C3" ><font color="#CB4335">Preprints</font></a></h2>-->

<!--	<table id="tbPreprints" width="100%">-->
<!--		-->
<!--	<tr>&nbsp</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--		<tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/EM-paste.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td>-->
<!--			<b>EM-Paste: EM-guided Cut-Paste with DALL-E Augmentation for Image-level Weakly Supervised Instance Segmentation</b><br>-->
<!--<br>-->
<!--&lt;!&ndash;		<br>&ndash;&gt;-->
<!--&lt;!&ndash;			<b>TL;DR:</b> A new paradigm for automatic context image generation at scale with DALL-E (generative models) for downstream takes (discriminative models). <br>&ndash;&gt;-->
<!--&lt;!&ndash;			<br>&ndash;&gt;-->
<!--			<b>Yunhao Ge</b><sup>*</sup>, Jiashu Xu<sup>*</sup>, Brian Nlong Zhao, Laurent Itti, Vibhav Vineet (*=equal contribution) <br>-->
<!--		<em>arXiv:2212.07629, 2022.</em>-->
<!--		<p></p>-->
<!--			<p>[<a href="https://arxiv.org/pdf/2212.07629.pdf" target="_blank">paper</a>]-->
<!--&lt;!&ndash;			[<a href="https://github.com/gyhandy/Visual-Reasoning-eXplanation" target="_blank">github</a>]&ndash;&gt;-->
<!--&lt;!&ndash;			[<a href="http://ilab.usc.edu/andy/vrx" target="_blank">website</a>]&ndash;&gt;-->
<!--&lt;!&ndash;			[<a href="https://youtu.be/ZzkpUrK-cRA" target="_blank">video</a>]&ndash;&gt;-->
<!--&lt;!&ndash;			[<a href="https://zhuanlan.zhihu.com/p/380264276" target="_blank">知乎</a>]&ndash;&gt;-->
<!--&lt;!&ndash;			[<a href="https://mp.weixin.qq.com/s/FhQsi7twHkGskcE5fshOxA" target="_blank">机器之心</a>]&ndash;&gt;-->
<!--&lt;!&ndash;			[<a href="https://mp.weixin.qq.com/s/DqYlLVMggNEWuN76-OXJmQ" target="_blank">AI科技评论</a>]&ndash;&gt;-->
<!--			</p>-->
<!--		</td>-->
<!--	</tr>-->

		



</table>

<h2><a id="C3" ><font color="#CB4335">Selected Publications</font></a> [<a href="https://scholar.google.com/citations?hl=en&user=QhjGr4oAAAAJ=en&user=QhjGr4oAAAAJ">Google Scholar</a>]</h2>

<table id="tbPublications" width="100%">

		
	<tr>
		<td width="306">
<!--		<img src="pics/3dcopy.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
			<img src="pics/vfc.gif" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>Visual Fact Checker: Enabling High-Fidelity Detailed Caption Generation</b><br>
<br>
<!--		<br>-->
<!--			<b>TL;DR:</b> A new paradigm for automatic context image generation at scale with DALL-E (generative models) for downstream takes (discriminative models). <br>-->
<!--			<br>-->
			<b>Yunhao Ge</b>, Xiaohui Zeng, Jacob Samuel Huffman, Tsung-Yi Lin, Ming-Yu Liu, Yin Cui <br>
		<b>CVPR 2024</b> (<em>IEEE Conference on Computer Vision and Pattern Recognition</em>).
		<p></p>
			<p>[<a href="https://arxiv.org/abs/2404.19752" target="_blank">paper</a>]
			[<a href="https://research.nvidia.com/labs/dir/vfc/assets/vfc_video.mp4" target="_blank">video</a>]
			[<a href="https://research.nvidia.com/labs/dir/vfc/" target="_blank">project page</a>]
<!--			[<a href="https://youtu.be/ZzkpUrK-cRA" target="_blank">video</a>]-->
<!--			[<a href="https://zhuanlan.zhihu.com/p/380264276" target="_blank">知乎</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/FhQsi7twHkGskcE5fshOxA" target="_blank">机器之心</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/DqYlLVMggNEWuN76-OXJmQ" target="_blank">AI科技评论</a>]-->
			</p>
		</td>
	</tr>
	
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
	
	<tr>
		<td width="306">
<!--		<img src="pics/3dcopy.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
			<img src="pics/dreamdistribution.gif" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>DreamDistribution: Prompt Distribution Learning for Text-to-Image Diffusion Models</b><br>
<br>
<!--		<br>-->
<!--			<b>TL;DR:</b> A new paradigm for automatic context image generation at scale with DALL-E (generative models) for downstream takes (discriminative models). <br>-->
<!--			<br>-->
			Brian Nlong Zhao, Yuhang Xiao<sup>*</sup>, Jiashu Xu<sup>*</sup>, Xinyang Jiang, Yifan Yang, Dongsheng Li, Laurent Itti, Vibhav Vineet<sup>†</sup>, <b>Yunhao Ge</b><sup>†</sup>
			(*=co-second author, †=equal contribution)<br>
		<b>arxiv:2312.14216, 2023</b>.
		<p></p>
			<p>[<a href="https://arxiv.org/pdf/2312.14216.pdf" target="_blank">paper</a>]
			[<a href="https://github.com/briannlongzhao/DreamDistribution" target="_blank">code</a>]
			[<a href="https://briannlongzhao.github.io/DreamDistribution/" target="_blank">project page</a>]
<!--			[<a href="https://youtu.be/ZzkpUrK-cRA" target="_blank">video</a>]-->
<!--			[<a href="https://zhuanlan.zhihu.com/p/380264276" target="_blank">知乎</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/FhQsi7twHkGskcE5fshOxA" target="_blank">机器之心</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/DqYlLVMggNEWuN76-OXJmQ" target="_blank">AI科技评论</a>]-->
			</p>
		</td>
	</tr>

	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>


	<tr>
		<td width="306">
<!--		<img src="pics/3dcopy.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
			<img src="pics/3D-copy-paste.gif" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>3D Copy-Paste: Physically-Plausible Object Insertion for Monocular 3D Detection</b><br>
<br>
<!--		<br>-->
<!--			<b>TL;DR:</b> A new paradigm for automatic context image generation at scale with DALL-E (generative models) for downstream takes (discriminative models). <br>-->
<!--			<br>-->
			<b>Yunhao Ge</b>, Hong-Xing Yu, Cheng Zhao, Yuliang Guo, Xinyu Huang, Liu Ren, Laurent Itti, Jiajun Wu <br>
		<b>NeurIPS 2023</b> (<em>Advances in Neural Information Processing Systems</em>).
		<p></p>
			<p>[<a href="https://arxiv.org/pdf/2312.05277.pdf" target="_blank">paper</a>]
			[<a href="https://github.com/gyhandy/3D-Copy-Paste" target="_blank">code</a>]
			[<a href="https://gyhandy.github.io/3D-Copy-Paste/" target="_blank">project page</a>]
<!--			[<a href="https://youtu.be/ZzkpUrK-cRA" target="_blank">video</a>]-->
<!--			[<a href="https://zhuanlan.zhihu.com/p/380264276" target="_blank">知乎</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/FhQsi7twHkGskcE5fshOxA" target="_blank">机器之心</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/DqYlLVMggNEWuN76-OXJmQ" target="_blank">AI科技评论</a>]-->
			</p>
		</td>
	</tr>

	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>

	<tr>
		<td width="306">
		<img src="pics/dalle-for-detection.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>DALL-E for Detection: Language-driven Compositional Image Synthesis for Object Detection</b><br>
			<b>Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation</b><br>
<br>
<!--		<br>-->
<!--			<b>TL;DR:</b> A new paradigm for automatic context image generation at scale with DALL-E (generative models) for downstream takes (discriminative models). <br>-->
<!--			<br>-->
			<b>Yunhao Ge</b><sup>*</sup>, Jiashu Xu<sup>*</sup>, Brian Nlong Zhao, Neel Joshi, Laurent Itti, Vibhav Vineet (*=equal contribution) <br>
		<em>arXiv:2206.09592, 2022.</em>
		<p></p>
			<p>[<a href="https://arxiv.org/pdf/2309.05956.pdf" target="_blank">paper(Beyond Generation)</a>]
				[<a href="https://arxiv.org/pdf/2206.09592.pdf" target="_blank">paper(DALL-E for Detection)</a>]
			[<a href="https://github.com/gyhandy/Text2Image-for-Detection" target="_blank">code</a>]
<!--			[<a href="http://ilab.usc.edu/andy/vrx" target="_blank">website</a>]-->
<!--			[<a href="https://youtu.be/ZzkpUrK-cRA" target="_blank">video</a>]-->
<!--			[<a href="https://zhuanlan.zhihu.com/p/380264276" target="_blank">知乎</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/FhQsi7twHkGskcE5fshOxA" target="_blank">机器之心</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/DqYlLVMggNEWuN76-OXJmQ" target="_blank">AI科技评论</a>]-->
			</p>
		</td>
	</tr>


	<tr>
		<td width="306">
<!--		<img src="pics/skill2.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
		<img src="pics/channel-LR.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>CLR: Channel-wise Lightweight Reprogramming for Continual Learning</b> <br>
					<br>
<!--			<b>TL;DR:</b> The first fully differentiable synthetic data pipeline that uses NeRFs in a closed-loop with a target application's loss function.-->
<!--			Neural-Sim generates data on-demand, with no human labor, to maximize accuracy for a target task. <br>-->
<!--			<br>-->
			<b>Yunhao Ge</b>, Yuecheng Li<sup>*</sup>, Shuo Ni<sup>*</sup>, Jiaping Zhao, Ming-Hsuan Yang, Laurent Itti
			(*=equal contribution as second author) <br>
		<b>ICCV 2023</b> (<em>International Conference on Computer Vision</em>).
<!--		<b>NeurIPS 2022 Workshop</b> (<em>ML Safety Workshop </em>).-->
		<p></p>
			<p>

				[<a href="https://arxiv.org/pdf/2307.11386.pdf" target="_blank">paper</a>]
				[<a href="https://github.com/gyhandy/Channel-wise-Lightweight-Reprogramming" target="_blank">code</a>]
<!--			[<a href="http://ilab.usc.edu/andy/skill" target="_blank">project page</a>]-->
			[<a href="http://ilab.usc.edu/andy/skill102" target="_blank">SKILL-102 Dataset</a>]
<!--			[<a href="https://viterbischool.usc.edu/news/2023/07/teaching-robots-to-teach-other-robots/" target="_blank">USC Viterbi Press</a>]-->
<!--								[<a paper and code coming soon</a>]-->
			</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>

	<tr>
		<td width="306">
<!--		<img src="pics/skill2.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
		<img src="pics/SKILL-m.gif" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>Lightweight Learner for Shared Knowledge Lifelong Learning</b> <br>
					<br>
<!--			<b>TL;DR:</b> The first fully differentiable synthetic data pipeline that uses NeRFs in a closed-loop with a target application's loss function.-->
<!--			Neural-Sim generates data on-demand, with no human labor, to maximize accuracy for a target task. <br>-->
<!--			<br>-->
			<b>Yunhao Ge</b>, Yuecheng Li<sup>*</sup>, Di Wu<sup>*</sup>, Ao Xu<sup>*</sup>, Adam M. Jones, Amanda Sofie Rios, Iordanis Fostiropoulos, Shixian wen, Po-Hsuan Huang, Zachary William Murdock, Gozde Sahin, Shuo Ni, Kiran Lekkala, Sumedh Anand Sontakke, Laurent Itti (*=equal contribution as second author) <br>
		<b>TMLR</b> (<em>Transactions on Machine Learning Research</em>).
<!--		<b>NeurIPS 2022 Workshop</b> (<em>ML Safety Workshop </em>).-->
		<p></p>
			<p>

			[<a href="https://openreview.net/pdf?id=Jjl2c8kWUc" target="_blank">paper</a>]
				[<a href="https://github.com/gyhandy/Shared-Knowledge-Lifelong-Learning" target="_blank">code</a>]
			[<a href="http://ilab.usc.edu/andy/skill" target="_blank">project page</a>]
			[<a href="http://ilab.usc.edu/andy/skill102" target="_blank">SKILL-102 Dataset</a>]
			[<a href="https://viterbischool.usc.edu/news/2023/07/teaching-robots-to-teach-other-robots/" target="_blank">USC Viterbi Press</a>]
<!--								[<a paper and code coming soon</a>]-->
			</p>
		</td>
	</tr>

		<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>

			<tr>
		<td width="306">
		<img src="pics/one-class-2.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>Building One-class Detector for Anything: Open-vocabulary Zero-shot OOD Detection Using Text-image Models</b><br>
<br>
<!--		<br>-->
<!--			<b>TL;DR:</b> A new paradigm for automatic context image generation at scale with DALL-E (generative models) for downstream takes (discriminative models). <br>-->
<!--			<br>-->
			<b>Yunhao Ge</b><sup>*</sup>, Jie Ren<sup>*</sup>, Jiaping Zhao, Kaifeng Chen, Andrew Gallagher,
		Laurent Itti, and Balaji Lakshminarayanan (*=equal contribution) <br>
		<em>Knowledge and Logical Reasoning workshop @ <b>ICML 2023</b> </em>
		<p></p>
			<p>[<a href="https://arxiv.org/pdf/2305.17207.pdf" target="_blank">paper</a>]
<!--			[<a href="https://github.com/gyhandy/Visual-Reasoning-eXplanation" target="_blank">github</a>]-->
<!--			[<a href="http://ilab.usc.edu/andy/vrx" target="_blank">website</a>]-->
<!--			[<a href="https://youtu.be/ZzkpUrK-cRA" target="_blank">video</a>]-->
<!--			[<a href="https://zhuanlan.zhihu.com/p/380264276" target="_blank">知乎</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/FhQsi7twHkGskcE5fshOxA" target="_blank">机器之心</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/DqYlLVMggNEWuN76-OXJmQ" target="_blank">AI科技评论</a>]-->
			</p>
		</td>
	</tr>



	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>

	<tr>
		<td width="306">
		<img src="pics/multi-modal.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>Improving Zero-shot Generalization and Robustness of Multi-modal Models</b> <br>
					<br>
<!--			<b>TL;DR:</b> The first fully differentiable synthetic data pipeline that uses NeRFs in a closed-loop with a target application's loss function.-->
<!--			Neural-Sim generates data on-demand, with no human labor, to maximize accuracy for a target task. <br>-->
<!--			<br>-->
			<b>Yunhao Ge</b><sup>*</sup>, Jie Ren<sup>*</sup>, Andrew Gallagher, Yuxiao Wang, Ming-Hsuan Yang,
		Hartwig Adam, Laurent Itti, Balaji Lakshminarayanan, and Jiaping Zhao (*=equal contribution) <br>

		<b>CVPR 2023</b> (<em>IEEE/ CVF International Conference on Computer Vision and Pattern Recognition</em>).
<!--		<b>NeurIPS 2022 Workshop</b> (<em>ML Safety Workshop </em>).-->
		<p></p>
			<p>

				[<a href="https://arxiv.org/pdf/2212.01758.pdf" target="_blank">paper</a>]
				[<a href="https://github.com/gyhandy/Hierarchy-CLIP" target="_blank">code</a>]
				[<a href="https://sites.google.com/usc.edu/hierarchy-clip/" target="_blank">project page</a>]
<!--								[<a paper and code coming soon</a>]-->
			</p>
		</td>
	</tr>


<!--<table id="tbPublications" width="100%">-->

	<tr>
		<td width="306">
		<img src="pics/neural-sim.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>Neural-Sim: Learning to Generate Training Data with NeRF</b> <br>
					<br>
<!--			<b>TL;DR:</b> The first fully differentiable synthetic data pipeline that uses NeRFs in a closed-loop with a target application's loss function.-->
<!--			Neural-Sim generates data on-demand, with no human labor, to maximize accuracy for a target task. <br>-->
<!--			<br>-->
			<b>Yunhao Ge</b>, Harkirat Behl<sup>*</sup>, Jiashu Xu<sup>*</sup>, Suriya Gunasekar, Neel Joshi,
		Yale Song, Xin Wang, Laurent Itti, and Vibhav Vineet (*=equal contribution as second author) <br>
		<b>ECCV 2022</b> (<em>European Conference on Computer Vision</em>).
		<p></p>
			<p>

								[<a href="https://arxiv.org/pdf/2207.11368.pdf" target="_blank">paper</a>]
				[<a href="https://github.com/gyhandy/Neural-Sim-NeRF" target="_blank">code</a>]
<!--								[<a paper and code coming soon</a>]-->
			</p>
		</td>
	</tr>



	<tr>
		<td width="306">
		<img src="pics/hve-2.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>Contributions of Shape, Texture, and Color in Visual Recognition</b><br>
<!--			<p style="color:#2E86C1 ";> <b>A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts</b> </p><br>-->
		<br>
<!--			<b>TL;DR:</b> Build a humanoid vision engine (HVE) that explicitly and separately computes shape, texture, and color features from images.-->
<!--			HVE can summarize and rank-order the contributions of the three features to object recognition. <br>-->
<!--			<br>-->
			<b>Yunhao Ge<sup>*</sup></b>, Yao Xiao<sup>*</sup>, Zhi Xu, Xingrui Wang, Laurent Itti (*=equal contribution) <br>
<!--		<em>European Conference on Computer Vision, </em>(<i><b>ECCV</b></i>), 2022.-->
			<b>ECCV 2022</b> (<em>European Conference on Computer Vision</em>).
		<p></p>
			<p>
			[<a href="https://arxiv.org/pdf/2207.09510.pdf" target="_blank">paper</a>]
				[<a href="https://github.com/gyhandy/Humanoid-Vision-Engine" target="_blank">code</a>]
<!--								[<a paper and code coming soon</a>]-->

<!--				[<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ge_A_Peek_Into_the_Reasoning_of_Neural_Networks_Interpreting_With_CVPR_2021_paper.pdf" target="_blank">paper</a>]-->
<!--			[<a href="https://github.com/gyhandy/Visual-Reasoning-eXplanation" target="_blank">github</a>]-->
<!--			[<a href="http://ilab.usc.edu/andy/vrx" target="_blank">website</a>]-->
<!--			[<a href="https://youtu.be/ZzkpUrK-cRA" target="_blank">video</a>]-->
<!--			[<a href="https://zhuanlan.zhihu.com/p/380264276" target="_blank">知乎</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/FhQsi7twHkGskcE5fshOxA" target="_blank">机器之心</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/DqYlLVMggNEWuN76-OXJmQ" target="_blank">AI科技评论</a>]-->
			</p>
		</td>
	</tr>

	<tr>
		<td width="306">
		<img src="pics/vrx-5.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts</b><br>
<!--			<p style="color:#2E86C1 ";> <b>A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts</b> </p><br>-->
		<br>
<!--			<b>TL;DR:</b> Takes a step towards mimicking the reasoning process of NNs and provide logical, concept-level-->
<!--						explanations for final model decisions. <br>-->
<!--			<br>-->
			<b>Yunhao Ge</b>, Yao Xiao, Zhi Xu, Meng Zheng, Srikrishna Karanam, Terrence Chen, Laurent Itti and Ziyan Wu  <br>
<!--		<em>IEEE/ CVF International Conference on Computer Vision and Pattern Recognition </em>(<i><b>CVPR</b></i>), 2021.-->
			<b>CVPR 2021</b> (<em>IEEE/ CVF International Conference on Computer Vision and Pattern Recognition</em>).
		<p></p>
			<p>[<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ge_A_Peek_Into_the_Reasoning_of_Neural_Networks_Interpreting_With_CVPR_2021_paper.pdf" target="_blank">paper</a>]
			[<a href="https://github.com/gyhandy/Visual-Reasoning-eXplanation" target="_blank">github</a>]
			[<a href="http://ilab.usc.edu/andy/vrx" target="_blank">project page</a>]
			[<a href="https://youtu.be/ZzkpUrK-cRA" target="_blank">video</a>]
			[<a href="https://zhuanlan.zhihu.com/p/380264276" target="_blank">知乎</a>]
			[<a href="https://mp.weixin.qq.com/s/FhQsi7twHkGskcE5fshOxA" target="_blank">机器之心</a>]
			[<a href="https://mp.weixin.qq.com/s/DqYlLVMggNEWuN76-OXJmQ" target="_blank">AI科技评论</a>]
			</p>
		</td>
	</tr>




	<tr>
		<td width="306">
		<img src="pics/GSL3.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td> <b>Zero-shot Synthesis with Group-Supervised Learning</b> <br>
		<br>
<!--		<b>TL;DR:</b> Using Controllable disentangled representation learning to simulate human knowledge factorization for imagination <br>-->
<!--			<br>-->
		<b>Yunhao Ge</b>, Sami Abu-El-Haija, Gan Xin and Laurent Itti  <br>
			<b>ICLR 2021</b> (<em>International Conference on Learning Representations</em>).
<!--		<em>International Conference on Learning Representations </em>(<i><b>ICLR</b></i>), 2021.-->
		<p></p>
		<p>[<a href="https://arxiv.org/pdf/2009.06586.pdf" target="_blank">paper</a>]
			[<a href="https://github.com/gyhandy/Group-Supervised-Learning" target="_blank">code</a>]
			[<a href="http://sami.haija.org/iclr21gsl" target="_blank">project page</a>]
			[<a href="http://ilab.usc.edu/datasets/fonts" target="_blank">Fonts Dataset</a>]
			[<a href="https://viterbischool.usc.edu/news/2021/07/usc-researchers-enable-ai-to-use-its-imagination/" target="_blank">USC Viterbi Press</a>]
			[<a href="https://zhuanlan.zhihu.com/p/364895887" target="_blank">知乎</a>]
			[<a href="https://mp.weixin.qq.com/s/o2HBYf3NF3UsMxUqkASdyg" target="_blank">AI科技评论</a>]<br>
			[<a  target="_blank" href=https://viterbischool.usc.edu/news/2021/07/usc-researchers-enable-ai-to-use-its-imagination/ target="_blank" rel="external"> USC News </a>]
			[<a  target="_blank" href=https://techxplore.com/news/2021-07-enabling-artificial-intelligence.html target="_blank" rel="external"> Tech Xplore </a>]
			[<a  target="_blank" href=https://www.technologynetworks.com/informatics/news/enabling-ai-to-use-its-imagination-350886 target="_blank" rel="external"> Technology Networks </a>]
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>


		<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>



<!--	<tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/ISL.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td>-->
<!--			<b>Invariant Structure Learning for Better Generalization and Causal Explainability</b><br>-->
<!--&lt;!&ndash;			<p style="color:#2E86C1 ";> <b>A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts</b> </p><br>&ndash;&gt;-->
<!--		<br>-->
<!--&lt;!&ndash;			<b>TL;DR:</b> ISL splits the data into different environments, and learns a structure that is invariant to the target across different&ndash;&gt;-->
<!--&lt;!&ndash;				environments by imposing a consistency constraint. <br>&ndash;&gt;-->
<!--&lt;!&ndash;			<br>&ndash;&gt;-->
<!--			<b>Yunhao Ge</b>, Sercan Ö. Arik, Jinsung Yoon, Ao Xu, Laurent Itti and Tomas Pfister  <br>-->
<!--		<b>TMLR</b> (<em>Transactions on Machine Learning Research</em>)-->
<!--		<p></p>-->
<!--			<p>[<a href="https://arxiv.org/pdf/2206.06469.pdf" target="_blank">paper</a>]-->
<!--&lt;!&ndash;			[<a href="https://github.com/gyhandy/Visual-Reasoning-eXplanation" target="_blank">github</a>]&ndash;&gt;-->
<!--&lt;!&ndash;			[<a href="http://ilab.usc.edu/andy/vrx" target="_blank">website</a>]&ndash;&gt;-->
<!--&lt;!&ndash;			[<a href="https://youtu.be/ZzkpUrK-cRA" target="_blank">video</a>]&ndash;&gt;-->
<!--&lt;!&ndash;			[<a href="https://zhuanlan.zhihu.com/p/380264276" target="_blank">知乎</a>]&ndash;&gt;-->
<!--&lt;!&ndash;			[<a href="https://mp.weixin.qq.com/s/FhQsi7twHkGskcE5fshOxA" target="_blank">机器之心</a>]&ndash;&gt;-->
<!--&lt;!&ndash;			[<a href="https://mp.weixin.qq.com/s/DqYlLVMggNEWuN76-OXJmQ" target="_blank">AI科技评论</a>]&ndash;&gt;-->
<!--			</p>-->
<!--		</td>-->
<!--	</tr>-->


<!--	<tr>&nbsp</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--	<tr>&nbsp</tr>-->

<!--			<tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/CIR-1.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td>-->
<!--			<b>Encouraging Disentangled and Convex Representation with Controllable Interpolation Regularization</b><br>-->
<!--&lt;!&ndash;			<p style="color:#2E86C1 ";> <b>A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts</b> </p><br>&ndash;&gt;-->
<!--		<br>-->
<!--&lt;!&ndash;		<b>TL;DR:</b> A simple yet efficient method: Controllable Interpolation Regularization (CIR), which creates a positive loop where the disentanglement and convexity can help each other  <br>&ndash;&gt;-->
<!--&lt;!&ndash;			<br>&ndash;&gt;-->
<!--			<b>Yunhao Ge</b>, Zhi Xu, Yao Xiao, Gan Xin, Yunkui Pang and Laurent Itti <br>-->
<!--		<b>WACV 2023</b> (<em>IEEE/CVF Winter Conference on Applications of Computer Vision</em>).-->
<!--		<p></p>-->
<!--		<p>[<a href="https://arxiv.org/pdf/2112.03163.pdf" target="_blank">paper</a>]</p>-->
<!--		</td>-->
<!--	</tr>-->

	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>




<!--		<tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/GraphAE.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td>-->
<!--			<b>Graph Autoencoder for Graph Compression and Representation Learning</b><br>-->
<!--&lt;!&ndash;			<p style="color:#2E86C1 ";> <b>A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts</b> </p><br>&ndash;&gt;-->
<!--		<br>-->
<!--&lt;!&ndash;		<b>TL;DR:</b> Multi-kernel Inductive Attention Graph Autoencoder (MIAGAE) utilizes the node similarity and graph structure to compress all nodes and edges as a whole.  <br>&ndash;&gt;-->
<!--&lt;!&ndash;			<br>&ndash;&gt;-->
<!--			<b>Yunhao Ge<sup>*</sup></b>, Yunkui Pang<sup>*</sup>, Linwei Li and Laurent Itti (*=equal contribution)<br>-->
<!--			<b>ICLR 2021 Workshop</b> (<em>Neural Compression: From Information Theory to Applications&#45;&#45;Workshop@ </em>).-->
<!--&lt;!&ndash;		<em>Neural Compression: From Information Theory to Applications&#45;&#45;Workshop@ </em>(<i><b>ICLR</b></i>), 2021.&ndash;&gt;-->
<!--		<p></p>-->
<!--		<p>[<a href="https://openreview.net/pdf?id=Bo2LZfaVHNi" target="_blank">paper</a>]-->
<!--			[<a href="https://github.com/Pangyk/Graph_AE" target="_blank">code</a>]-->
<!--			[<a href="http://ilab.usc.edu/datasets/i2sg" target="_blank">Img2SceneGraph</a>]</p>-->
<!--			<p><strong style="color:blue">Spotlight Presentation</strong></p>-->
<!--		</td>-->
<!--	</tr>-->


    <tr>
		<td width="306">
		<img src="pics/eccv.gif" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td> <b>Pose Augmentation: Class-agnostic Object Pose Transformation for Object Recognition</b> <br>
			<br>
<!--			<b>TL;DR:</b> Eliminate-add generator to explicitly disentangle pose from object identity by maximizing pose entropy<br>-->
<!--			<br>-->
		<b>Yunhao Ge</b>, Jiaping Zhao, Laurent Itti  <br>
		<b>ECCV 2020</b> (<em>European Conference on Computer Vision</em>).
<!--		<em>European Conference on Computer Vision</em> (<i><b>ECCV</b></i>), 2020.-->
		<p></p>
		<p>[<a href="https://arxiv.org/pdf/2003.08526.pdf" target="_blank">paper</a>]
			[<a href="https://github.com/gyhandy/Pose-Augmentation" target="_blank">github</a>]
			[<a href="https://youtu.be/WHAFj9KXRFY" target="_blank">video-1min</a>]
			[<a href="https://youtu.be/9N8eyOmCWh4" target="_blank">video-10min</a>]</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>

<!--	<tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/Beneficial.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>Beneficial Perturbation Network for designing general adaptive artificial intelligence systems</b> <br>-->
<!--&lt;!&ndash;		<b>TL;DR:</b>Allowing a single network to learn potentially unlimited parallel input to output mappings, and to switch on the fly&ndash;&gt;-->
<!--&lt;!&ndash;			between them at runtime.<br>&ndash;&gt;-->
<!--			<br>-->
<!--		Shixian Wen, Amanda Rios<sup>*</sup>, <b>Yunhao Ge<sup>*</sup></b> and Laurent Itti (*=equal contribution as second author) <br>-->
<!--			<b>TNNLS 2021</b> (<em> IEEE Transactions on Neural Networks and Learning Systems </em>).-->
<!--&lt;!&ndash;		<em> IEEE Transactions on Neural Networks and Learning Systems </em>(<i><b>TNNLS</b></i>), 2021.&ndash;&gt;-->
<!--		<p></p>-->
<!--		<p>[<a href="https://arxiv.org/pdf/2009.13954.pdf" target="_blank">paper</a>]-->
<!--		</td>-->




    <tr>
		<td width="306">
		<img src="pics/project11.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td> <b>Unpaired MR to CT Synthesis with Explicit Structural Constrained Adversarial Learning</b> <br>
			 <br>
		<b>Yunhao Ge<sup>*</sup></b>, Dongming Wei<sup>*</sup>, Zhong Xue, Yiqiang Zhan, Xiang Zhou, Qian Wang and Shu Liao (*=equal contribution)<br>
			<b>ISBI 2019</b> (<em>IEEE International Symposium on Biomedical Imaging</em>).
<!--		<em>IEEE International Symposium on Biomedical Imaging</em> (<i><b>ISBI</b></i>), 2019.-->
		<p></p>
		<p>[<a href="files/UNPAIRED.pdf" target="_blank">paper</a>]
            [<a href="https://github.com/gyhandy/Unpaired-Cross-modality-Image-Synthesis" target="_blank">code</a>]</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>


<!--    <tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/SLIR-MedIA.jpg" width="285px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>Synthesis and inpainting-based MR-CT registration for image-guided thermal ablation of liver tumors</b> <br>-->
<!--			<br>-->
<!--		Dongming Wei, Sahar Ahmad, Jiayu Huo, Wen Peng, <b>Yunhao Ge</b>, Zhong Xue, Pew-Thian Yap, Wentao Li, Dinggang Shen, Qian Wang <br>-->
<!--			<b>MICCAI 2019</b> (<em>International Conference on Medical Image Computing and Computer-Assisted Intervention</em>).-->
<!--&lt;!&ndash;		<em>International Conference on Medical Image Computing and Computer-Assisted Intervention</em> (<i><b>MICCAI</b></i>), 2019.&ndash;&gt;-->
<!--		<p></p>-->
<!--		<p>[<a href="https://arxiv.org/pdf/1907.13020.pdf" target="_blank">paper</a>]</p>-->
<!--		</td>-->
<!--	</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->

    <tr>
		<td width="306">
		<img src="pics/SPIE1.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td> <b>Unpaired Whole-body MR to CT Synthesis with Correlation Coefficient Constrained Adversarial Learning</b> <br>
			<br>
		<b>Yunhao Ge</b>, Zhong Xue, Yiqiang Zhan, Xiang Zhou and Shu Liao <br>
		<b>SPIE 2019</b> (<em>SPIE-Medical Imaging</em>).
<!--		<em>SPIE-Medical Imaging</em> (<i><b>SPIE</b></i>), 2019.-->
		<p></p>
		<p>[<a href="files/SPIE.pdf" target="_blank">paper</a>]
            [<a href="https://github.com/gyhandy/Unpaired-Cross-modality-Image-Synthesis" target="_blank">code</a>]</p>

		<p><strong style="color:blue">Oral Presentation</strong></p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>


<!--    <tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/project3.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>A Real-time Gesture Prediction System Using Neural Networks and Multimodal Fusion-->
<!--					based on Data Glove</b> <br>-->
<!--			<br>-->
<!--		<b>Yunhao Ge</b>, Bin Li and Weixin Yan <br>-->
<!--		<em>IEEE International Conference on Advanced Computational Intelligence</em> (<i><b>ICACI</b></i>), 2018.-->
<!--		<p></p>-->
<!--		<p>[<a href="files/Real.pdf" target="_blank">paper</a>]</p>-->
<!--			<p><strong style="color:blue">Oral Presentation</strong></p>-->
<!--		</td>-->
<!--	</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->

<!--    <tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/HHnet.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>HH-Net: Image driven microscope fast auto-focus with deep neural network</b><br>-->
<!--			<br>-->
<!--		<b>Yunhao Ge</b>, Bin Li, Yanzheng Zhao and Weixin Yan <br>-->
<!--		<em>International Conference on Biomedical Engineering and Technology</em> (<i><b>ICBET</b></i>), 2019.-->
<!--		<p></p>-->
<!--		<p>[<a href="files/HH-net.pdf" target="_blank">paper</a>]-->
<!--        </p>-->
<!--		</td>-->
<!--	</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->

<!--    <tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/project4.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>Melanoma Segmentation and Classification in Clinical Images Using Deep Learning</b><br>-->
<!--			<br>-->
<!--		<b>Yunhao Ge</b>, Bin Li and Weixin Yan <br>-->
<!--		<em>ACM International Conference on Machine Learning and Computing</em> (<i><b>ICMLC</b></i>), 2018.-->
<!--		<p></p>-->
<!--		<p>[<a href="files/Melanoma.pdf" target="_blank">paper</a>][<a  target="_blank" href="https://dl.acm.org/citation.cfm?id=3195164">Paper Link</a>]-->
<!--        </p>-->
<!--		</td>-->
<!--	</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->

<!--    <tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/project5.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>Benign and Malignant Mammographic Image Classification based on Convolutional-->
<!--					Neural Networks</b><br>-->
<!--			<br>-->
<!--		Bin Li, <b>Yunhao Ge</b>, Yanzheng Zhao, Enguang Guan and Weixin Yan <br>-->
<!--		<em>ACM International Conference on Machine Learning and Computing</em> (<i><b>ICMLC</b></i>), 2018.-->
<!--		<p></p>-->
<!--		<p>[<a href="files/Benign.pdf" target="_blank">paper</a>][<a  target="_blank" href="https://dl.acm.org/citation.cfm?id=3195163&dl=ACM&coll=DL">Paper Link</a>]-->
<!--        </p>-->
<!--		</td>-->
<!--	</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->

<!--    <tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/project.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>Effect of Mechanical Error on Dual-Wedge Laser Scanning System and Error Correction</b><br>-->
<!--			<br>-->
<!--		<b>Yunhao Ge</b>, Jihao Liu, Fenfen Xue, Enguang Guan, Weixin Yan and Yanzheng Zhao <br>-->
<!--		<i><b>Applied Optics</b></i>, 2018.-->
<!--		<p></p>-->
<!--		<p>[<a href="files/Effect.pdf" target="_blank">paper</a>][<a  target="_blank" href="https://www.osapublishing.org/ao/abstract.cfm?uri=ao-57-21-6047">Paper Link</a>]-->
<!--        </p>-->
<!--		</td>-->
<!--	</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->

<!--    <tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/project8.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>Dynamic Drive Performances of the Bionic Suction Cup Actuator Based on Shape Memory Alloy</b><br>-->
<!--		<b>Yunhao Ge</b>, Jihao Liu, Bin Li, Weixin Yan and Yanzheng Zhao  <br>-->
<!--		<em>Intelligent Robotics and Applications</em> (<i><b>ICIRA</b></i>), 2017.-->
<!--		<p></p>-->
<!--		<p>[<a href="files/Dynamic.pdf" target="_blank">paper</a>][<a  target="_blank" href="https://link.springer.com/chapter/10.1007%2F978-3-319-65289-4_2">Paper Link</a>]-->
<!--        </p>-->
<!--		</td>-->
<!--	</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->


<!--	<tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/project7.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td><strong>Interactive Experience across Modern Age and Tradition: Application of Arduino in Shadow Play</strong><br>-->
<!--            <em>Kun Yin, <b>Yunhao Ge</b>, Heshan Liu and Yongquan Yin  <br></em>-->
<!--            <em>Advances in Mechatronics and Machinery </em>.-->
<!--			<br>[<a target="_blank" href="files/Interactive.pdf">PDF</a>] [<a  target="_blank" href="https://www.scientific.net/AMM.868.242">Paper Link</a>]-->
<!--&lt;!&ndash;		<br>[<a  target="_blank" href="https://www.scientific.net/AMM.868.242">Paper Link</a>]&ndash;&gt;-->
<!--        <br><br>-->
<!--		</td>-->
<!--	</tr>-->



</table>
<br>
<h2><a id="C4" ><font color="#CB4335">Intern & Work Experience</font></a></h2>

      <h4> Google Research, Los Angeles, USA  (May. 2022 - Dec. 2022) </h4>
	  <ul>
        <li>Position: Student Researcher in <a  target="_blank" href=https://research.google/ target="_blank" rel="external"> Google Research </a></li>


		  <li>Supervisor: Dr. <a  target="_blank" href=https://scholar.google.com/citations?user=j64_S3EAAAAJ&hl=en rel="external"> Jiaping Zhao </a>,
			  Dr. <a  target="_blank" href=https://jessieren.github.io/ target="_blank" rel="external"> Jie Ren </a>,
			  Dr. <a  target="_blank" href=http://www.gatsby.ucl.ac.uk/~balaji/ target="_blank" rel="external"> Balaji Lakshminarayanan </a>
        and Prof. <a  target="_blank" href=http://faculty.ucmerced.edu/mhyang/ target="_blank" rel="external"> Ming-Hsuan Yang</a></li>
        <li>Project: Improving Zero-shot Generalization and Robustness of Multi-modal Models --> <b>Accepted by CVPR 2023</b> </li>
    </ul>

      <h4> Google Cloud AI, Mountain View, USA  (Aug. 2021 - May 2022) </h4>
	  <ul>
        <li>Position: Student Researcher in <a  target="_blank" href="https://cloud.google.com/" target="_blank" rel="external">Google Cloud AI</a></li>
		  <li>Supervisor: Dr. <a  target="_blank" href=https://sercanarik.com/ rel="external"> Sercan Arik </a>
        and Dr. <a  target="_blank" href=https://sites.google.com/view/jinsungyoon target="_blank" rel="external"> Jinsung Yoon </a></li>
        <li>Project: Causal Explainable AI --> <b>Accepted by TMLR</b> </li>
    </ul>

      <h4> Microsoft Research, Redmond, USA  (May. 2021 - Aug. 2021) </h4>
	  <ul>
        <li>Position: Research Intern in <a  target="_blank" href="https://www.microsoft.com/en-us/research/research-area/computer-vision/?facet%5Btax%5D%5Bmsr-research-area%5D%5B0%5D=13562&sort_by=most-recent" target="_blank" rel="external">Microsoft Research Computer Vision Group</a></li>
		  <li>Supervisor: Dr. <a  target="_blank" href=http://vibhavvineet.info/ target="_blank" rel="external"> Vibhav Vineet </a>
            and Dr. <a  target="_blank" href=https://neelj.com/ target="_blank" rel="external"> Neel Joshi </a></li>
        <li>Project: On-demand training data generation with Neural Rendering --> <b>Accepted by ECCV 2022</b> </li>
    </ul>

<!--    <img id="school_logo" src="./pics/uII.png">-->
      <h4> UII America, Inc, Boston, USA  (May. 2020 - Aug. 2020) </h4>
	  <ul>
        <li>Position: Research Intern in <a  target="_blank" href="https://www.uii-ai.com/en/" target="_blank" rel="external">UII America</a></li>
        <li>Supervisor: Dr. <a  target="_blank" href=http://wuziyan.com/ target="_blank" rel="external"> Ziyan Wu </a>
            and Dr. <a  target="_blank" href=https://karanams.github.io/ target="_blank" rel="external"> Srikrishna Karanam </a></li>
        <li>Project: Explain reasoning logic of NN --> <b>Accepted by CVPR 2021</b> </li>
    </ul>

<!--    <img id="school_logo" src="./pics/Flexiv.png">-->
          <!--<h4> The University of North Carolina at Chapel Hill, NC, USA  & <br> Shanghai United ImagingIntelligence Co., Ltd, China  (Jun. 2018 - Nov. 2018)</br> </h4>-->
          <h4> Flexiv Ltd, Shanghai, China  (May. 2019 - Aug. 2019) </h4>
          <ul>
            <li>Position: Computer Vision Engineer in <a  target="_blank" href="http://flexiv.com/" target="_blank" rel="external">Flexiv Robotics</a></li>
            <li>Supervisor: Prof.<a  target="_blank" href=http://webcache.googleusercontent.com/search?q=cache:http://mvig.sjtu.edu.cn/ target="_blank" rel="external"> Cewu Lu</a> and
				Dr. <a  target="_blank" href=https://www.linkedin.com/in/shuyun-chung-67b5337a/ target="_blank" rel="external"> Shuyun Chong</a>
			</li>
            <li>Project: Robotics adaptive massage based on human pose detection and tracking </li>
            <br/>
            <br/>
            <td width="306">
            <img src="pics/masage.gif" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">
            </td>
      </ul>

<!--    <img id="school_logo" src="./pics/uII.png">-->
      <h4> United Imaging Intelligence Co., Ltd, Shanghai, China  (Jun. 2018 - Apr. 2019) </h4>
	  <ul>
        <li>Position: Research Intern in <a  target="_blank" href="https://www.uii-ai.com/en/" target="_blank" rel="external">United Imaging Intelligence</a></li>
        <li>Supervisor: Prof.<a  target="_blank" href=https://scholar.google.com/citations?user=v6VYQC8AAAAJ&hl=zh-CN target="_blank" rel="external"> Dinggang Shen</a>
        and Dr. <a  target="_blank" href=https://scholar.google.com/citations?user=INL-unYAAAAJ&hl=en target="_blank" rel="external"> Shu Liao </a></li>
        <li>Project: Unpaired MR to CT image synthesis --> <b>Accepted by ISBI 2019 and MICCAI 2019</b>  </li>
    </ul>


<br>

<h2><font color="#CB4335">Scholarships</font></h2>
      <ul>
		<li>
          Aug. 2022 <b>Amazon ML Fellowship (2022-2023)</b></li>
		<li>
			Apr. 2022 <b>Annenberg Project Grant for simulating human imagination</b>, awarded annually to 10 PhD students across USC
			for high impact projects</li>
		<li>
          Aug. 2019 <b>Annenberg Graduate Fellowship at University of Southern California</b></li>
        <li>
          Sep. 2017 <b>National Scholarship(Graduate)</b>, (highest honor for graduates) <strong style="color:blue">top 1% nationwide</strong></li>
        <li>
          Sep. 2015 <b>National Scholarship(Undergraduate)</b>, (highest honor for undergraduates, top 2% nationwide) </li>
		<li>
          May. 2018 <b>KaiYuan Motivational Scholarship</b> <strong style="color:blue">top 0.5% in Shanghai Jiao Tong University</strong></li>
        <li>
          Sep. 2015 <b>Presidential Scholarship</b>, (highest honor in Shandong University) <strong style="color:blue">top 0.2% in Shandong University</strong></li>
        <li>
          Sep. 2015 <b>BaoGang Excellent student Scholarship</b>, (4 Places per year at Shandong University) </li>
		<li>
          Sep. 2015 &amp; Sep 2014 &amp; Sep 2015</b> <b>First Prize Scholarship</b> (Top 6% in China,three-year continuous)</li>
      </ul>

<br>
<!--<h2><font color="#CB4335">Honors and Awards</font></h2>-->
<!--      <ul>-->
<!--        <li>-->
<!--          Aug. 2017 <b>The First Prize</b> 2017 ROBOMASTER <strong style="color:blue">The World’s Leading Robotics Competition</strong> (Responsible for the design-->
<!--of electronic control in robotics)[<a  target="_blank" href="https://github.com/gyhandy/Andy">Code-T_Infantry</a>]</li>-->
<!--		<li>-->
<!--          Aug. 2017 <b>Rank 1st</b> (preliminary competition) , Tianchi: Precision medical competition-Artificial Intelligence Aided-->
<!--genetic risk prediction of diabetes [<a  target="_blank" href="https://github.com/gyhandy/Diabetes-Mellitus">Code-Pred_diabetes</a>]</li>-->
<!--        <li>-->
<!--          Oct. 2015 <b>The First Prize</b> 9th international college students ican innovation and entrepreneurship contest</li>-->
<!--        <li>-->
<!--          Oct. 2015 <b>The Special prize(Top 1%)</b> 6th National University Students Process Equipment Practice and Innovation Competition</li>-->
<!--		<li>-->
<!--          Aug. 2015 <b>The Silver prize</b> 8th national college students in energy saving social practice and science and technology competition </li>-->
<!--      </ul>-->

<!--<br>-->


<h2><font color="#CB4335">Academic Service</font></h2>
Reviewer of the following conferences/journals:
<br>
<br>
NeurIPS 2023, 2022, 2021<br>
CVPR 2023, 2022<br>
ECCV 2022<br>
ICCV 2023, 2021<br>
ICLR 2023, 2022<br>
ICML 2022<br>
WACV 2023<br>


<!--2021 IEEE CVPR WORKSHOP ON FAIR, DATA EFFICIENT AND TRUSTED COMPUTER VISION<br>-->
IEEE Transactions on Medical Imaging (TMI)<br>
IEEE Access<br>
Applied Optics<br>

<br>





<!--  <h2><font color="#CB4335">Software & Patents</font></h2>-->
<!--<table id="tbPublications" width="100%">-->


<!--	<tr>-->
<!--        <td width="306">-->
<!--        <img src="pics/uspatent.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">-->
<!--        </td>-->
<!--        <td><strong>Systems and methods for image processing</strong><br>-->
<!--            <em> S Liao, <b>GE Yunhao</b>, WEI Dongming<br></em>-->
<!--            <em>US Patent App. 16/729,303</em>.-->
<!--&lt;!&ndash;        <br>[<a href="https://ieeexplore.ieee.org/abstract/document/8413124/">Paper</a>]&ndash;&gt;-->
<!--        <br><br>-->
<!--        </td>-->
<!--    </tr>-->

<!--    <tr>-->
<!--        <td width="306">-->
<!--        <img src="pics/software.jpg" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">-->
<!--        </td>-->
<!--        <td><strong>Pulmonary Nodular Assisted Detection System Based on AI(V1.0)</strong><br>-->
<!--            <em> Bin Li, <b>Yunhao Ge</b><br></em>-->
<!--            <em>2018SR037095</em>.-->
<!--&lt;!&ndash;        <br>[<a href="https://ieeexplore.ieee.org/abstract/document/8413124/">Paper</a>]&ndash;&gt;-->
<!--        <br><br>-->
<!--        </td>-->
<!--    </tr>-->

<!--    <tr>-->
<!--        <td width="306">-->
<!--        <img src="pics/patent1.jpg" width="285px" height= "170px" style="box-shadow: 4px 4px 8px #888">-->
<!--        </td>-->
<!--        <td><strong>A Two-Layer Barrier Free Parking Equipment Based on Bionic Manipulator</strong><br>-->
<!--            <em><b>Yunhao Ge</b>, Shangze Yang, Zheng Zhang, Weixin Yan and Yanzheng Zhao <br></em>-->
<!--            <em>CN201610712048</em>.-->
<!--&lt;!&ndash;        <br>[<a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w3/Xu_Dual-Mode_Vehicle_Motion_CVPR_2018_paper.pdf">Paper</a>]&ndash;&gt;-->
<!--        <br><br>-->
<!--        </td>-->
<!--    </tr>-->

<!--    <tr>-->
<!--        <td width="306">-->
<!--        <img src="pics/patent2.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">-->
<!--        </td>-->
<!--        <td><strong>A Double Decker Parking Equipment based on Shear Lifting Mechanism and Hydraulic Mechanism</strong><br>-->
<!--            <em><b>Yunhao Ge</b>, Xulong Zhou, Peng Liu and Yanzheng Zhao <br></em>-->
<!--            <em>CN201610704408</em>.-->
<!--&lt;!&ndash;        <br>[<a href="https://github.com/gyhandy/publication/blob/master/A%20Real-time%20Gesture%20Prediction%20System%20Using%20Neural%20Networks%20and%20Multimodal%20Fusion%20based%20on%20data%20glove.pdf">Paper</a>] &ndash;&gt;-->
<!--        <br><br>-->
<!--        </td>-->
<!--    </tr>-->
<!--</table>-->
<!--<br>-->
<!--
 <h3>Academice Service</h3>
<hr>
<table id="tbActivities" border="0" width="100%">
	Reviewer of the following conferences/journals:
    <p>IEEE Transactions on Multimedia (TMM) </p>
    <p>IEEE International Conference on Computer Vision (ICCV 2017)</p>
    <p>ACM Multimedia Conference (MM 2017)</p>
    <p>IEEE International Conference on Image Processing (ICIP 2017)</p>
    <p>AAAI Conference on Artificial Intelligence (AAAI 2016)</p>
</table> 
-->

<!-- <h2>OpenCourse Achievements</h2>-->
<!--      <h4> DeepLearning.ai</h4>-->
<!--      <ul>-->
<!--        <li>Neural Networks and Deep Learning</li>-->
<!--        <li>Improving Deep Neural Networks</li>-->
<!--        <li>Structuring Machine Learning Projects </li>-->
<!--		<li>Convolutional Neural Networks </li>-->
<!--  </ul> -->
<!--<br> -->
<!--
<h4>Links</h4>
<strong>
<a href="http://cs231n.stanford.edu/">CS231n@Stanford</a><br>
<a href="http://pytorch.org/">PyTorch</a><br>

</strong>
-->
<hr>

<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=n&d=F_s9W3nU5OYqMT1Q8FiSEnLSHBjVFBhCTQoaSexG7Mk'></script>
<p align="center"><font color="#999999">Last update: Sep. 25, 2023</font></p>



</body>

</html>
